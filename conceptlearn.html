---
layout: default
title: Yangqing Jia
---

<h3>Human Concept Learning</h3>

<p> Note: This page will be updated with more information before the NIPS conference. </p>

<h4> Overview </h4>


<h4> Mechanical Turk </h4>

Following prior art conventions, we presented the concept learning task in the AMT HITs as a task of helping "Mr Frog", who speaks a different language, to identify more objects that he wants by looking at his examples. The users are presented with 5 images from an underlying category, and are asked to pick out images that belong to the same category, from a set of 20 candidate images. An example query could be checked out [linked to be added].

<h4> Data and Testing Protocol </h4>

All the images in our AMT experiments come from the ILSVRC 2010 testing set. Thus, if you would like to train your own image classifiers, follow the same protocol of the ImageNet challenge: use training and validation data to train the classifier and/or tune hyperparameters.


<h4> Citations </h4>

<p> Please consider citing the following paper if you use our dataset or mechanical turk interface in your research. </p>

<ul>
<li>Y. Jia, J. Abbott, J. Austerweil, T. Griffiths, T. Darrell. Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies. NIPS 2013.</li>
</ul>
