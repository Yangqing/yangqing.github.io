<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/decaf-features.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</strong><br />
      J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell. arXiv preprint.<br />
      <a href="http://arxiv.org/abs/1310.1531">[ArXiv Link]</a>
      <a href="http://decaf.berkeleyvision.org/">[Live Demo]</a>
      <a href="https://github.com/UCB-ICSI-Vision-Group/decaf-release/">[Software]</a>
      <a href="http://www.eecs.berkeley.edu/~jiayq/decaf_pretrained/">[Pretrained ImageNet Model]</a>
    </p>
    <p class="abstract-text">
    We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. We also released the software and pre-trained network to do large-scale image classification.
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/mrFrog.jpg" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</strong><br />
      Y Jia, J Abbott, J Austerweil, T Griffiths, T Darrell. NIPS 2013.
      <a href="#">[PDF coming soon]</a>
    </p>
    <p class="abstract-text">
    It is marvelous that human can learn concept from a small number of examples, a challenge many existing machine vision systems fail to do. We present a system combining computer vision and cogscience to model such human behavior, as well as a new dataset for future experientation on human concept learning.
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/iccv13_ta.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Latent Task Adaptation with Large-scale Hierarchies</strong><br />
      Y Jia, T Darrell. ICCV 2013.
      <a href="#">[PDF coming soon]</a>
    </p>
    <p class="abstract-text">
    How do we adapt our ImageNet classifiers to accurately classify just giraffes and bears on a zoo trip? We proposed a novel framework that benefits from big training data and adaptively adjusts itself for subcategory test scenarios. 
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/iccv13_sa.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Category Independent Object-level Saliency Detection</strong><br />
      Y Jia, M Han. ICCV 2013.
      <a href="#">[PDF coming soon]</a>
    </p>
    <p class="abstract-text">
    We proposed a simple yet efficient approach to combine high-level object models and low-level appearance information to perform saliency detection that identifies foreground objects.
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/icml13.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>On Compact Codes for Spatially Pooled Features</strong><br />
      Y Jia, O Vinyals, T Darrell. ICML 2013.
      <a href="http://jmlr.org/proceedings/papers/v28/jia13.pdf">[PDF]</a>
      <a href="assets/pdf/icml13_poster.pdf">[poster]</a>
      <a href=http://arxiv.org/abs/1301.5348>[ICLR workshop version]</a>
    </p>
    <p class="abstract-text">
      We analyzed the connection between codebook size and accuracy with the Nystrom sampling theory, and showed how this leads to better pooling-aware codebook learning methods.
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/nips12.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Learning with Recursive Perceptual Representations</strong><br />
      O Vinyals, Y Jia, L Deng, T Darrell. NIPS 2012.
      <a href="assets/pdf/nips12_rsvm.pdf">[PDF]</a>
      <a href="assets/pdf/nips12_rsvm_poster.pdf">[Poster]</a>
    </p>
    <p class="abstract-text">
      We proposed R2SVM, an efficient algorithm to recursively learn deep nonlinear models by stacking linear SVMs with random projections.
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/cvpr12.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Beyond Spatial Pyramids: Receptive Field Learning for Pooled Image Features</strong><br />
      Y Jia, C Huang, T Darrell. CVPR 2012.
      <a href="assets/pdf/cvpr12_pooling.pdf">[PDF]</a>
      <a href="assets/pdf/cvpr12_pooling_slides.pdf">[Slides]</a>
      <a href="assets/pdf/cvpr12_pooling_poster.pdf">[Poster]</a>
    </p>
    <p class="abstract-text">
      We showed the suboptimality of spatial pyramids in feature pooling, and proposed an efficient way to learn task-dependent receptive fields for better pooled features.
    </p>
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/uai12.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Factorized Multi-modal Topic Model</strong><br />
      S Virtanen, Y Jia, A Klami, T Darrell. UAI 2012.
      <a href="assets/pdf/uai12_factorize.pdf">[PDF]</a>
      <!--<a href="wikipedia.html">[Dataset]</a> -->
    </p>
    <p class="abstract-text">
      We factorized the information contained in corresponding image and text with a novel HDP-based topic model that automatically learns both shared and private topics.
    </p>
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/nips11.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Heavy-tailed Distances for Gradient Based Image Descriptors</strong><br />
      Y Jia, T Darrell. NIPS 2011.
      <a href="assets/pdf/nips11_gcl.pdf">[PDF]</a>
      <a href="assets/pdf/nips11_gcl_supp.pdf">[Supplementary Material]</a>
      <a href="assets/pdf/nips11_gcl_poster.pdf">[Poster]</a>
    </p>
    <p class="abstract-text">
      We examined the heavy-tailed noise distribution of gradient-based image descriptors, and proposed a new distance metric that yields higher feature matching performances.
    </p>
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/iccv11.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Learning Cross-modality Similarity for Multinomial Data</strong><br />
      Y Jia, M Salzmann, T Darrell. ICCV 2011
      <a href="assets/pdf/iccv11_mm.pdf">[PDF]</a>
      <a href="assets/pdf/iccv11_mm_poster.pdf">[Poster]</a>
      <!--<a href="wikipedia.html">[Dataset]</a> -->
    </p>
    <p class="abstract-text">
      We propose a novel approach based on topic models and the Markov random field to capture the semantic relationships between documents from multiple modalities. 
    </p>
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/papers/iccv11-b3do.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>A Category-level 3-D Database: Putting the Kinect to Work</strong><br />
      A Janoch, S Karayev, Y Jia, J Barron, M Fritz, K Saenko, T Darrell. ICCV-CDC4CV workshop 2011
      <a href="assets/pdf/iccv11_kinect.pdf">[PDF]</a>
      <a href="http://kinectdata.com/">[Dataset]</a>
    </p>
    <p class="abstract-text">
      We presented a dataset of color and depth image pairs collected from the Kinect sensor, gathered in real domestic and ofÔ¨Åce environments, for research on object-level recognition with multimodal sensor input.
    </p>
  </div>
</div>
